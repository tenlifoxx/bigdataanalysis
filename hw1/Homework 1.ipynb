{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Homework 1 (Due: Mar. 17, 2021 (11:59 PM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: 홍민기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student ID: 2015313098\n",
    "Late submission Days used here: 0 (if there is, please modify here)\n",
    "\n",
    "[Please SUBMIT (1) YOUR IPYNB AND (2) PDF (please use FILE/DOWNLOAD AS/PDF or PRINT PREVIEW/PRINT AS PDF with \"printed output\") TO iCampus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For this homework you cannot use the python library scikit-learn (sklearn). \n",
    "You can use the python package BeautifulSoup to parse web pages.\n",
    "\n",
    "In this assignment you will retrieve and parse webpages. The text file \"urls.txt\" contains a list of urls for the webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage.\n",
    "\n",
    "Note: For all questions, the words should be converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 1 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Parse the first webpage document to retrieve the text enclosed in paragraph tags, find the words that end in \"ing\", and count how many times each word appears. \n",
    "\n",
    "Sort these words in decreasing order of frequency, and write the words (along with their corresponding frequencies) in an output file named \"Q1_Part1.txt\". The most frequent word should appear at the top and the least frequent word at the end, and the format of the output file should be:\n",
    "word TAB frequency\n",
    "\n",
    "Example:\n",
    "\n",
    "sorting\t10\n",
    "training\t8\n",
    "broadening\t6\n",
    "extracting\t3\n",
    "evergrowing\t2\n",
    "coming\t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning 5\n",
      "mining 3\n",
      "beijing 2\n",
      "computing 2\n",
      "accounting 1\n",
      "analyzing 1\n",
      "applying 1\n",
      "becoming 1\n",
      "being 1\n",
      "breaking 1\n",
      "changing 1\n",
      "combining 1\n",
      "creating 1\n",
      "describing 1\n",
      "developing 1\n",
      "drawing 1\n",
      "during 1\n",
      "emerging 1\n",
      "enabling 1\n",
      "everything 1\n",
      "extracting 1\n",
      "finding 1\n",
      "formulating 1\n",
      "growing 1\n",
      "including 1\n",
      "managing 1\n",
      "preparing 1\n",
      "presenting 1\n",
      "reflecting 1\n",
      "training 1\n",
      "turing 1\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "### Get Paragraphs from URL and lower alphabets\n",
    "r = urllib.request.urlopen('https://en.wikipedia.org/wiki/Data_science').read()\n",
    "soup = BeautifulSoup(r)\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "pText = ''\n",
    "for p in paragraphs:\n",
    "    pText = pText + '\\n' + p.get_text().strip()\n",
    "    \n",
    "pText = pText.lower()\n",
    "\n",
    "    \n",
    "### from paragraphs get pattern, ~ing\n",
    "pattern = re.compile(r\"\\w*ing\\W\")\n",
    "words = re.findall(pattern, pText)\n",
    "words_new = []\n",
    "for word in words:\n",
    "    fixed = word[:-1]\n",
    "    if fixed not in words_new:\n",
    "        words_new.append(fixed)\n",
    "words = words_new\n",
    "\n",
    "\n",
    "### Split paragraphs into words, get rid of puntuation marks\n",
    "pattern = re.compile(r\"\\w+\")\n",
    "pTextList = re.findall(pattern, pText)\n",
    "\n",
    "\n",
    "### Count frequency of words end with ~ing\n",
    "count = {}\n",
    "for word in words:\n",
    "    cnt = 0\n",
    "    for w in pTextList:\n",
    "        if w == word: cnt = cnt + 1   \n",
    "    count[word] = cnt\n",
    "\n",
    "### Sort by Frequency\n",
    "count_sort = sorted(count.items())\n",
    "count_sort.sort(reverse=True, key=lambda x:x[1])\n",
    "\n",
    "### Write to Text\n",
    "f = open(\"Q1_Part1.txt\", 'w')\n",
    "for key, values in count_sort:\n",
    "    if values == 0: break\n",
    "    line = key + ' ' + str(values) + '\\n'\n",
    "    f.write(line)\n",
    "f.close()\n",
    "\n",
    "f = open(\"Q1_Part1.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    print(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 2 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.\n",
    "\n",
    "Repeat Part 1, but before counting, remove the stop words given in the file \"stop_words.txt\". The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q1_Part2.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning 5\n",
      "mining 3\n",
      "beijing 2\n",
      "computing 2\n",
      "accounting 1\n",
      "analyzing 1\n",
      "applying 1\n",
      "breaking 1\n",
      "changing 1\n",
      "combining 1\n",
      "creating 1\n",
      "describing 1\n",
      "developing 1\n",
      "drawing 1\n",
      "emerging 1\n",
      "enabling 1\n",
      "extracting 1\n",
      "finding 1\n",
      "formulating 1\n",
      "growing 1\n",
      "including 1\n",
      "managing 1\n",
      "preparing 1\n",
      "presenting 1\n",
      "reflecting 1\n",
      "training 1\n",
      "turing 1\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "### Make Stop Word List\n",
    "f = open(\"stop_words.txt\", 'r')\n",
    "stop_words = []\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    stop_words.append(line)\n",
    "f.close()\n",
    "\n",
    "\n",
    "### Get Paragraphs from URL and lower alphabets\n",
    "r = urllib.request.urlopen('https://en.wikipedia.org/wiki/Data_science').read()\n",
    "soup = BeautifulSoup(r)\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "pText = ''\n",
    "for p in paragraphs:\n",
    "    pText = pText + '\\n' + p.get_text().strip()\n",
    "    \n",
    "pText = pText.lower()\n",
    "\n",
    "    \n",
    "### from paragraphs get pattern, ~ing\n",
    "pattern = re.compile(r\"\\w*ing\\W\")\n",
    "words = re.findall(pattern, pText)\n",
    "words_new = []\n",
    "for word in words:\n",
    "    fixed = word[:-1]\n",
    "    if fixed not in words_new:\n",
    "        words_new.append(fixed)\n",
    "words = words_new\n",
    "\n",
    "### Split paragraphs into words, get rid of puntuation marks\n",
    "pattern = re.compile(r\"\\w+\")\n",
    "pTextList = re.findall(pattern, pText)\n",
    "\n",
    "\n",
    "### Get rid of stop words from list\n",
    "for sw in stop_words:\n",
    "    if sw in pTextList:\n",
    "        pTextList.remove(sw)\n",
    "\n",
    "### Count frequency of words end with ~ing\n",
    "count = {}\n",
    "for word in words:\n",
    "    cnt = 0\n",
    "    for w in pTextList:\n",
    "        if w == word: cnt = cnt + 1\n",
    "    \n",
    "    count[word] = cnt\n",
    "\n",
    "### Sort by Frequency\n",
    "count_sort = sorted(count.items())\n",
    "count_sort.sort(reverse=True, key=lambda x:x[1])\n",
    "\n",
    "### Write to Text\n",
    "f = open(\"Q1_Part2.txt\", 'w')\n",
    "for key, values in count_sort:\n",
    "    if values == 0: break\n",
    "    line = key + ' ' + str(values) + '\\n'\n",
    "    f.write(line)\n",
    "f.close()\n",
    "\n",
    "f = open(\"Q1_Part2.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    print(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Again, parse the first webpage document, but this time find and count all outgoing links to other webpages, and write the output to a file named \"Q2.txt\", with each url on a new line.\n",
    "\n",
    "The format of the output file should: number of outgoing urls in the first line, followed by each url on a new line. \n",
    "For example, \n",
    "4\n",
    "https://eng.skku.edu/eng/edu/education.do\n",
    "https://eng.skku.edu/eng/Research/industry/researchStory.do\n",
    "https://eng.skku.edu/eng/Univ-Industry/Research-Business-Found/FactsandFigures.do\n",
    "https://eng.skku.edu/eng/CampusLife/support/employment.do\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "https://arxiv.org/list/cs.LG/recent\n",
      "https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&action=edit\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit\n",
      "http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext\n",
      "https://doi.org/10.1145%2F2500499\n",
      "https://api.semanticscholar.org/CorpusID:6107147\n",
      "https://web.archive.org/web/20141109113411/http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext\n",
      "http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/\n",
      "https://web.archive.org/web/20140102194117/http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/\n",
      "https://www.springer.com/book/9784431702085\n",
      "https://doi.org/10.1007%2F978-4-431-65950-1_3\n",
      "https://books.google.com/books?id=oGs_AQAAIAAJ\n",
      "https://web.archive.org/web/20170320193019/https://books.google.com/books?id=oGs_AQAAIAAJ\n",
      "https://doi.org/10.1126%2Fscience.1170411\n",
      "https://api.semanticscholar.org/CorpusID:9743327\n",
      "http://www.datascienceassn.org/about-data-science\n",
      "https://www.oreilly.com/library/view/doing-data-science/9781449363871/ch01.html\n",
      "https://medriscoll.com/post/4740157098/the-three-sexy-skills-of-data-geeks\n",
      "https://flowingdata.com/2009/06/04/rise-of-the-data-scientist/\n",
      "https://benfry.com/phd/dissertation/2.html\n",
      "https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/\n",
      "https://web.archive.org/web/20190620184935/https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/\n",
      "https://www.statisticsviews.com/article/nate-silver-what-i-need-from-statisticians/\n",
      "http://priceonomics.com/whats-the-difference-between-data-science-and/\n",
      "https://doi.org/10.1145%2F2500499\n",
      "https://api.semanticscholar.org/CorpusID:6107147\n",
      "https://statmodeling.stat.columbia.edu/2013/11/14/statistics-least-important-part-data-science/\n",
      "https://www.datasciencecentral.com/profiles/blogs/data-science-without-statistics-is-possible-even-desirable\n",
      "http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf\n",
      "https://www2.isye.gatech.edu/~jeffwu/publications/fazhan.pdf\n",
      "https://www.mdpi.com/2504-2289/2/2/14\n",
      "https://doi.org/10.3390%2Fbdcc2020014\n",
      "https://doi.org/10.1145%2F3076253\n",
      "https://doi.org/10.1145%2F3076253\n",
      "http://www2.isye.gatech.edu/~jeffwu/presentations/datascience.pdf\n",
      "https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/\n",
      "https://www.stat.purdue.edu/~wsc/\n",
      "https://magazine.amstat.org/blog/2016/06/01/datascience-2/\n",
      "https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century\n",
      "https://www.nsf.gov/pubs/2005/nsb0540/\n",
      "https://www.forbes.com/sites/gilpress/2013/08/19/data-science-whats-the-half-life-of-a-buzzword/\n",
      "https://www.forbes.com/sites/peterpham/2015/08/28/the-impacts-of-big-data-that-you-may-not-have-heard-of/\n",
      "https://towardsdatascience.com/how-data-science-will-impact-future-of-businesses-7f11f5699c4d\n",
      "https://sites.engineering.ucsb.edu/~shell/che210d/python.pdf\n",
      "https://cran.r-project.org/doc/FAQ/R-FAQ.html#What-is-R_003f\n",
      "https://www.wired.com/2014/07/a-drag-and-drop-toolkit-that-lets-anyone-create-interactive-maps/\n",
      "https://en.wikipedia.org/w/index.php?title=Template:Data&action=edit\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&oldid=1011117061\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q2374463\n",
      "https://commons.wikimedia.org/wiki/Category:Data_science\n",
      "https://ar.wikipedia.org/wiki/%D8%B9%D9%84%D9%85_%D8%A7%D9%84%D8%A8%D9%8A%D8%A7%D9%86%D8%A7%D8%AA\n",
      "https://az.wikipedia.org/wiki/Veril%C9%99nl%C9%99r_elmi_(Data_Science)\n",
      "https://bn.wikipedia.org/wiki/%E0%A6%89%E0%A6%AA%E0%A6%BE%E0%A6%A4%E0%A7%8D%E0%A6%A4_%E0%A6%AC%E0%A6%BF%E0%A6%9C%E0%A7%8D%E0%A6%9E%E0%A6%BE%E0%A6%A8\n",
      "https://ca.wikipedia.org/wiki/Ci%C3%A8ncia_de_les_dades\n",
      "https://cs.wikipedia.org/wiki/Data_science\n",
      "https://de.wikipedia.org/wiki/Data_Science\n",
      "https://et.wikipedia.org/wiki/Andmeteadus\n",
      "https://el.wikipedia.org/wiki/%CE%95%CF%80%CE%B9%CF%83%CF%84%CE%AE%CE%BC%CE%B7_%CE%B4%CE%B5%CE%B4%CE%BF%CE%BC%CE%AD%CE%BD%CF%89%CE%BD\n",
      "https://es.wikipedia.org/wiki/Ciencia_de_datos\n",
      "https://eu.wikipedia.org/wiki/Datu_zientzia\n",
      "https://fa.wikipedia.org/wiki/%D8%B9%D9%84%D9%85_%D8%AF%D8%A7%D8%AF%D9%87%E2%80%8C%D9%87%D8%A7\n",
      "https://fr.wikipedia.org/wiki/Science_des_donn%C3%A9es\n",
      "https://ko.wikipedia.org/wiki/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4\n",
      "https://hy.wikipedia.org/wiki/%D5%8F%D5%BE%D5%B5%D5%A1%D5%AC%D5%B6%D5%A5%D6%80%D5%AB_%D5%A3%D5%AB%D5%BF%D5%B8%D6%82%D5%A9%D5%B5%D5%B8%D6%82%D5%B6\n",
      "https://hi.wikipedia.org/wiki/%E0%A4%86%E0%A4%81%E0%A4%95%E0%A4%A1%E0%A4%BC%E0%A4%BE_%E0%A4%B5%E0%A4%BF%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%A8\n",
      "https://id.wikipedia.org/wiki/Ilmu_data\n",
      "https://it.wikipedia.org/wiki/Scienza_dei_dati\n",
      "https://he.wikipedia.org/wiki/%D7%9E%D7%93%D7%A2_%D7%94%D7%A0%D7%AA%D7%95%D7%A0%D7%99%D7%9D\n",
      "https://kk.wikipedia.org/wiki/%D0%94%D0%B5%D1%80%D0%B5%D0%BA%D1%82%D0%B5%D1%80_%D1%82%D1%83%D1%80%D0%B0%D0%BB%D1%8B_%D2%93%D1%8B%D0%BB%D1%8B%D0%BC\n",
      "https://lv.wikipedia.org/wiki/Datu_m%C4%81c%C4%ABba\n",
      "https://mk.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%B7%D0%B0_%D0%BF%D0%BE%D0%B4%D0%B0%D1%82%D0%BE%D1%86%D0%B8\n",
      "https://ms.wikipedia.org/wiki/Sains_data\n",
      "https://my.wikipedia.org/wiki/%E1%80%A1%E1%80%81%E1%80%BB%E1%80%80%E1%80%BA%E1%80%A1%E1%80%9C%E1%80%80%E1%80%BA%E1%80%9E%E1%80%AD%E1%80%95%E1%80%B9%E1%80%95%E1%80%B6%E1%80%95%E1%80%8A%E1%80%AC\n",
      "https://nl.wikipedia.org/wiki/Datawetenschap\n",
      "https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9\n",
      "https://pl.wikipedia.org/wiki/Danologia\n",
      "https://pt.wikipedia.org/wiki/Ci%C3%AAncia_de_dados\n",
      "https://ru.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%BE_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85\n",
      "https://simple.wikipedia.org/wiki/Data_science\n",
      "https://fi.wikipedia.org/wiki/Datatiede\n",
      "https://ta.wikipedia.org/wiki/%E0%AE%A4%E0%AE%B0%E0%AE%B5%E0%AF%81_%E0%AE%85%E0%AE%B1%E0%AE%BF%E0%AE%B5%E0%AE%BF%E0%AE%AF%E0%AE%B2%E0%AF%8D\n",
      "https://th.wikipedia.org/wiki/%E0%B8%A7%E0%B8%B4%E0%B8%97%E0%B8%A2%E0%B8%B2%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5\n",
      "https://tr.wikipedia.org/wiki/Veri_bilimi\n",
      "https://uk.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%BF%D1%80%D0%BE_%D0%B4%D0%B0%D0%BD%D1%96\n",
      "https://ur.wikipedia.org/wiki/%DA%88%DB%8C%D9%B9%D8%A7_%D8%B3%D8%A7%D8%A6%D9%86%D8%B3\n",
      "https://vi.wikipedia.org/wiki/Khoa_h%E1%BB%8Dc_d%E1%BB%AF_li%E1%BB%87u\n",
      "https://zh-yue.wikipedia.org/wiki/%E6%95%B8%E6%93%9A%E7%A7%91%E5%AD%B8\n",
      "https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q2374463#sitelinks-wikipedia\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "### Open and read page from url\n",
    "r = urllib.request.urlopen('https://en.wikipedia.org/wiki/Data_science').read()\n",
    "soup = BeautifulSoup(r)\n",
    "\n",
    "\n",
    "### Find links that has 'http'\n",
    "links = soup.findAll('a')\n",
    "urls = []\n",
    "for link in links:\n",
    "    linkUrl = link.get('href')\n",
    "    if linkUrl is not None:\n",
    "        if \"http\" in linkUrl:\n",
    "            urls.append(linkUrl)\n",
    "\n",
    "\n",
    "### Print out result\n",
    "f = open(\"Q2.txt\", 'w')\n",
    "f.write(str(len(urls)) + '\\n')\n",
    "for link in urls:\n",
    "    f.write(link + '\\n')\n",
    "f.close()\n",
    "\n",
    "f = open(\"Q2.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    print(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 1 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "1. Retrieve and parse multiple web pages. The text file \"urls.txt\" contains a list of webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage and parse it as specified below. \n",
    "\n",
    "2. For each webpage document do the following:\n",
    "    1. Retrieve all text enclosed in paragraph tags. \n",
    "    2. Convert the text to lowercase. \n",
    "    3. Strip out punctuation. Note: if you use translate() with string.punctuation, then it may not strip out all characters. Use a regular expression involving \\W to strip out all non alpha-numeric characters.\n",
    "    4. Tokenize into words based on whitespace separation.\n",
    "\n",
    "3. Find the number of unique words in each webpage document. \n",
    "\n",
    "4. Find the Length of each webpage document. The length of a document is defined as the total number of words in the document (not just unique words).\n",
    "\n",
    "5. For each of the following words: “statistics”, “analytics”, “data”, and “science”, \n",
    "    a. Find Term Frequency (tf). \n",
    "    The term frequency (tf) of a term (word) is defined as the number of times that term t occurs in document d, \n",
    "    divided by the total number of words in the document. \n",
    "    The tf of a word depends on the document under consideration. \n",
    "    \n",
    "    b. Find Inverse Document Frequency (idf).\n",
    "    The inverse document frequency of a word is the logarithmically scaled inverse fraction of the documents that \n",
    "    contain the word, obtained by dividing the total number of documents by the number of documents containing the \n",
    "    term, and then taking the logarithm of that ratio.\n",
    "    The idf of a word doesn't depend on any documnet in which the word is present. \n",
    "    To calculate the idf, you will have to use the log function. The base for the log function must be e.\n",
    "    \n",
    "    c. Find tf-idf. \n",
    "    The tf-idf of a word is the product of the term frequency of the word in document d, and its inverse document \n",
    "    frequency. \n",
    "    The tf-idf of a word depends on the document under consideration. \n",
    "    \n",
    "    Reference: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "    \n",
    "The output should be written to an output file named \"Q3_Part1.txt\".\n",
    "\n",
    "The format of the output file is as shown below:\n",
    "\n",
    "1. Number of unique words in documents: [702, 723, 280]\n",
    "\n",
    "2. Length of documents: [1711, 1928, 563]\n",
    "\n",
    "3. tf\n",
    "    statistics: [0.0070134424313267095, 0.0025933609958506223, 0.0]\n",
    "    analytics: [0.0029222676797194622, 0.0031120331950207467, 0.0]\n",
    "    data: [0.056107539450613676, 0.05446058091286307, 0.0]\n",
    "    science: [0.03798947983635301, 0.011410788381742738, 0.028419182948490232]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.510825623766\n",
    "    analytics: 0.510825623766\n",
    "    data: 0.223143551314\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0028437061936282715, 0.0010515173965460695, 0.0]\n",
    "    analytics: [0.0011848775806784465, 0.0012618208758552834, 0.0]\n",
    "    data: [0.022749649549026172, 0.022081865327467459, 0.0]\n",
    "    science: [0.0, 0.0, 0.0]\n",
    "   \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of unique words in documents: [473, 1975, 727, 913, 1170]\n",
      "2. Length of documents: [1049, 6306, 1950, 2339, 3681]\n",
      "3. tf\n",
      "statistics: [0.017159199237368923, 0.0003171582619727244, 0.002564102564102564, 0.0, 0.0032599837000814994]\n",
      "analytics: [0.0009532888465204957, 0.0028544243577545195, 0.003076923076923077, 0.00042753313381787086, 0.0010866612333604998]\n",
      "data: [0.06482364156339371, 0.04551221059308595, 0.055384615384615386, 0.0, 0.04862809019288237]\n",
      "science: [0.03813155386081983, 0.0017443704408499842, 0.011282051282051283, 0.0017101325352714834, 0.016571583808747622]\n",
      "4. idf\n",
      "statistics: 0.22314355131420976\n",
      "analytics: 0.0\n",
      "data: 0.22314355131420976\n",
      "science: 0.0\n",
      "5. tf-idf\n",
      "statistics: [0.0038289646555345813, 7.07718209052362e-05, 0.0005721629520877173, 0.0, 0.0007274443400626235]\n",
      "analytics: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "data: [0.014464977587575084, 0.010155756299901397, 0.012358719765094695, 0.0, 0.010851044739267467]\n",
      "science: [0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import math\n",
    "\n",
    "\n",
    "def getParagraph_split(url):\n",
    "    r = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    pText = ''\n",
    "    for p in paragraphs:\n",
    "        pText = pText + '\\n' + p.get_text().strip()\n",
    "    \n",
    "    pText = pText.lower()\n",
    "    \n",
    "    pText = re.sub(r'\\W+', ' ', pText)\n",
    "    \n",
    "    return pText.split()\n",
    "\n",
    "\n",
    "def getLength(list):\n",
    "    return len(list)\n",
    "\n",
    "\n",
    "def countUnique(list):\n",
    "    tem_set = set(list)\n",
    "    \n",
    "    return len(tem_set)\n",
    "\n",
    "\n",
    "def getTf(word, list):\n",
    "    cnt = 0\n",
    "    for w in list:\n",
    "        if w == word:\n",
    "            cnt = cnt + 1\n",
    "    \n",
    "    return cnt/getLength(list)\n",
    "\n",
    "\n",
    "def getIdf(tfList):\n",
    "    cnt = 0\n",
    "    for i in tfList:\n",
    "        if i == 0:\n",
    "            cnt = cnt + 1\n",
    "    \n",
    "    return math.log((len(tfList)/(len(tfList)-cnt)))\n",
    "\n",
    "\n",
    "def getTI(tf, idf):\n",
    "    mul_num = []\n",
    "    for num in tf:\n",
    "        mul_num.append(num * idf)\n",
    "        \n",
    "    return mul_num\n",
    "\n",
    "\n",
    "\n",
    "### Get URL List from text file\n",
    "f = open(\"urls.txt\", 'r')\n",
    "urlList = []\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    urlList.append(line)\n",
    "f.close()\n",
    "\n",
    "\n",
    "### Get the number of unique words, lenghs, and tfs from urls\n",
    "unique = []\n",
    "length = []\n",
    "statistics_tf = []\n",
    "analytics_tf = []\n",
    "data_tf = []\n",
    "science_tf = []\n",
    "\n",
    "\n",
    "for url in urlList:\n",
    "    p = getParagraph_split(url)\n",
    "    unique.append(countUnique(p))\n",
    "    length.append(getLength(p))\n",
    "    statistics_tf.append(getTf('statistics', p))\n",
    "    analytics_tf.append(getTf('analytics', p))\n",
    "    data_tf.append(getTf('data', p))\n",
    "    science_tf.append(getTf('science', p))\n",
    "    \n",
    "### Get idfs\n",
    "statistics_idf = getIdf(statistics_tf)\n",
    "analytics_idf = getIdf(analytics_tf)\n",
    "data_idf = getIdf(data_tf)\n",
    "science_idf = getIdf(science_tf)\n",
    "\n",
    "\n",
    "### Get tf-idfs\n",
    "statistics_ti = getTI(statistics_tf, statistics_idf)\n",
    "analytics_ti = getTI(analytics_tf, analytics_idf)\n",
    "data_ti = getTI(data_tf, data_idf)\n",
    "science_ti = getTI(science_tf, science_idf)\n",
    "\n",
    "\n",
    "f = open(\"Q3_Part1.txt\", 'w')\n",
    "f.write(\"1. Number of unique words in documents: \")\n",
    "f.write(str(unique))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.write(\"2. Length of documents: \")\n",
    "f.write(str(length))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.write(\"3. tf\\n\")\n",
    "f.write(\"statistics: \")\n",
    "f.write(str(statistics_tf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"analytics: \")\n",
    "f.write(str(analytics_tf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"data: \")\n",
    "f.write(str(data_tf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"science: \")\n",
    "f.write(str(science_tf))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.write(\"4. idf\\n\")\n",
    "f.write(\"statistics: \")\n",
    "f.write(str(statistics_idf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"analytics: \")\n",
    "f.write(str(analytics_idf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"data: \")\n",
    "f.write(str(data_idf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"science: \")\n",
    "f.write(str(science_idf))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.write(\"5. tf-idf\\n\")\n",
    "f.write(\"statistics: \")\n",
    "f.write(str(statistics_ti))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"analytics: \")\n",
    "f.write(str(analytics_ti))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"data: \")\n",
    "f.write(str(data_ti))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"science: \")\n",
    "f.write(str(science_ti))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.close()\n",
    "\n",
    "f = open(\"Q3_Part1.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    print(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Repeat Part 1, but first remove the stop words given in the file \"stop_words.txt\".  \n",
    "The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q3_Part2.txt\".\n",
    "Note: The length of document, in this case, will not include stop words. Similarly, the number of unique words in documents, and the calculation of tf, idf, tf-idf should be done after removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Sample output for the file \"urls.txt\"\n",
    "\n",
    "1. Number of unique words in documents: [596, 600, 231]\n",
    "\n",
    "2. Length of documents: [1020, 1079, 357]  \n",
    "\n",
    "3. tf\n",
    "    statistics: [0.011764705882352941, 0.004633920296570899, 0.0]\n",
    "    analytics: [0.004901960784313725, 0.005560704355885079, 0.0]\n",
    "    data: [0.09411764705882353, 0.09731232622798888, 0.0]\n",
    "    science: [0.06372549019607843, 0.020389249304911955, 0.04481792717086835]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.405465108108\n",
    "    analytics: 0.405465108108\n",
    "    data: 0.405465108108\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0047701777424489925, 0.0018788929940137366, 0.0]\n",
    "    analytics: [0.0019875740593537469, 0.002254671592816484, 0.0]\n",
    "    data: [0.03816142193959194, 0.039456752874288473, 0.0]\n",
    "    science: [0.0, 0.0, 0.0] \n",
    "      \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of unique words in documents: [431, 1929, 689, 873, 1128]\n",
      "2. Length of documents: [965, 6125, 1829, 2235, 3528]\n",
      "3. tf\n",
      "statistics: [0.01865284974093264, 0.00032653061224489796, 0.002733734281027884, 0.0, 0.003401360544217687]\n",
      "analytics: [0.0010362694300518134, 0.0029387755102040815, 0.0032804811372334607, 0.00044742729306487697, 0.0011337868480725624]\n",
      "data: [0.07046632124352331, 0.046857142857142854, 0.0590486604702023, 0.0, 0.05073696145124717]\n",
      "science: [0.04145077720207254, 0.0017959183673469388, 0.01202843083652269, 0.0017897091722595079, 0.017290249433106575]\n",
      "4. idf\n",
      "statistics: 0.22314355131420976\n",
      "analytics: 0.0\n",
      "data: 0.22314355131420976\n",
      "science: 0.0\n",
      "5. tf-idf\n",
      "statistics: [0.004162263133322047, 7.286320042912972e-05, 0.00061001517581796, 0.0, 0.0007589916711367679]\n",
      "analytics: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "data: [0.015724105170327733, 0.010455869261580115, 0.013176327797667935, 0.0, 0.011321625761123456]\n",
      "science: [0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import math\n",
    "\n",
    "\n",
    "def getParagraph_split(url):\n",
    "    r = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    pText = ''\n",
    "    for p in paragraphs:\n",
    "        pText = pText + '\\n' + p.get_text().strip()\n",
    "    \n",
    "    pText = pText.lower()\n",
    "    \n",
    "    pText = re.sub(r'\\W+', ' ', pText)\n",
    "    \n",
    "    return pText.split()\n",
    "\n",
    "\n",
    "def stopWords(list, stop_words):\n",
    "    for sw in stop_words:\n",
    "        if sw in list:\n",
    "            list.remove(sw)\n",
    "            \n",
    "    return list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getLength(list):\n",
    "    return len(list)\n",
    "\n",
    "\n",
    "def countUnique(list):\n",
    "    tem_set = set(list)\n",
    "    \n",
    "    return len(tem_set)\n",
    "\n",
    "\n",
    "def getTf(word, list):\n",
    "    cnt = 0\n",
    "    for w in list:\n",
    "        if w == word:\n",
    "            cnt = cnt + 1\n",
    "    \n",
    "    return cnt/getLength(list)\n",
    "\n",
    "\n",
    "def getIdf(tfList):\n",
    "    cnt = 0\n",
    "    for i in tfList:\n",
    "        if i == 0:\n",
    "            cnt = cnt + 1\n",
    "    \n",
    "    return math.log((len(tfList)/(len(tfList)-cnt)))\n",
    "\n",
    "\n",
    "def getTI(tf, idf):\n",
    "    mul_num = []\n",
    "    for num in tf:\n",
    "        mul_num.append(num * idf)\n",
    "        \n",
    "    return mul_num\n",
    "\n",
    "\n",
    "\n",
    "### Make Stop Word List\n",
    "f = open(\"stop_words.txt\", 'r')\n",
    "stop_words = []\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    stop_words.append(line)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "### Get URL List from text file\n",
    "f = open(\"urls.txt\", 'r')\n",
    "urlList = []\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    urlList.append(line)\n",
    "f.close()\n",
    "\n",
    "\n",
    "### Get the number of unique words, lenghs, and tfs from urls\n",
    "unique = []\n",
    "length = []\n",
    "statistics_tf = []\n",
    "analytics_tf = []\n",
    "data_tf = []\n",
    "science_tf = []\n",
    "\n",
    "\n",
    "for url in urlList:\n",
    "    p = getParagraph_split(url)\n",
    "    p = stopWords(p, stop_words)\n",
    "    unique.append(countUnique(p))\n",
    "    length.append(getLength(p))\n",
    "    statistics_tf.append(getTf('statistics', p))\n",
    "    analytics_tf.append(getTf('analytics', p))\n",
    "    data_tf.append(getTf('data', p))\n",
    "    science_tf.append(getTf('science', p))\n",
    "\n",
    "### Get idfs\n",
    "statistics_idf = getIdf(statistics_tf)\n",
    "analytics_idf = getIdf(analytics_tf)\n",
    "data_idf = getIdf(data_tf)\n",
    "science_idf = getIdf(science_tf)\n",
    "\n",
    "\n",
    "### Get tf-idfs\n",
    "statistics_ti = getTI(statistics_tf, statistics_idf)\n",
    "analytics_ti = getTI(analytics_tf, analytics_idf)\n",
    "data_ti = getTI(data_tf, data_idf)\n",
    "science_ti = getTI(science_tf, science_idf)\n",
    "\n",
    "\n",
    "f = open(\"Q3_Part2.txt\", 'w')\n",
    "f.write(\"1. Number of unique words in documents: \")\n",
    "f.write(str(unique))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.write(\"2. Length of documents: \")\n",
    "f.write(str(length))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.write(\"3. tf\\n\")\n",
    "f.write(\"statistics: \")\n",
    "f.write(str(statistics_tf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"analytics: \")\n",
    "f.write(str(analytics_tf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"data: \")\n",
    "f.write(str(data_tf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"science: \")\n",
    "f.write(str(science_tf))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.write(\"4. idf\\n\")\n",
    "f.write(\"statistics: \")\n",
    "f.write(str(statistics_idf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"analytics: \")\n",
    "f.write(str(analytics_idf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"data: \")\n",
    "f.write(str(data_idf))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"science: \")\n",
    "f.write(str(science_idf))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.write(\"5. tf-idf\\n\")\n",
    "f.write(\"statistics: \")\n",
    "f.write(str(statistics_ti))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"analytics: \")\n",
    "f.write(str(analytics_ti))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"data: \")\n",
    "f.write(str(data_ti))\n",
    "f.write(\"\\n\")\n",
    "f.write(\"science: \")\n",
    "f.write(str(science_ti))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "f.close()\n",
    "\n",
    "f = open(\"Q3_Part2.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    print(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
